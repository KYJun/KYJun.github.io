---
layout: post
title: "강화학습 튜토리얼 03 : 마코프 결정 과정 (MDP)"
categories: rl
---
Markov Decision Process : 마코프 결정 과정

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
>3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> 피드백은 항상 환영합니다.

### 선택의 기로
현재 우리가 설정한 목표는 **Agent가 관측할 수 있는 환경의 범위 내에서 가장 큰 Reward를 얻는 Action을 선택**하는 것입니다. Agent의 입장에서 고려할 수 있는 정보는 다음과 같습니다.

- 현재 Agent가 놓인 상황
- 다음에 취할 수 있는 Action 목록
- 각 Action에서 얻게 되는 Reward

하지만 여기서 다시 한 번 Reward에 주목해 봅니다. Agent의 목적은 목표를 달성하는 것이며, Reward는 선택에 대한 평가 지표가 될 수 있다고 가정했습니다. 그러나 Reward로 충분할까요?

<br>

### Return

목표는 단 한 번의 Action으로 달성하지 못하는 경우가 많을 겁니다. 

따라서, 목표 달성을 위해서는 단순히 다음 단계의 Reward만이 아니라 현재 선택이 안겨다줄 미래의 이득 전체를 고려하는 게 합당할 것입니다.

즉, Agent는 **앞으로 얻을 수 있는 모든 Reward의 총합**을 고려해야 한다는 이야기죠.

<br>

하지만 단순히 Reward를 더하기만 하면 여전히 문제가 남아있습니다.

아럐 예시를 살펴보도록 하죠.

다음 두 가지 상황 중 어느 것이 더 좋은 것일까요? 

A : 한번에 1000만원 받기

B : 10년 동안 매년 100만원 받기

앞서 정의한 내용에 따르면, 두 선택지 모두 Reward의 총합이 1000만원이 되어 같은 이득을 주는 선택이 됩니다.

하지만 우리는 지금 받는 백만원과 일년 뒤에 받는 백만원이 같은 가치가 아님을 알고 있습니다. 물가 상승률, 이자율 등등 두 선택에는 명백한 차이가 존재하죠. 

그렇다면 기계가 이 차이를 인식하게 하려면 어떻게 해야 할까요?

<br>

여기서 **Discounting Factor**가 등장하게 됩니다. Discounting Factor는 말 그대로 각 단계별로 Reward 값을 감소시키는 0과 1 사이의 수치입니다. 매 단계 n마다 Reward는 discounting factor의 n 제곱을 곱한 뒤 합쳐지게 됩니다. 이렇게 한다면 10년 뒤의 백만원은 보다 작은 숫자로 바뀔 것입니다.

이렇게 해서 구해진 최종 식은 다음과 같습니다.

![return](/assets/03_mdp/return.png)

> 각 단계별 Reward에 discounting factor(gamma)가 곱해진 뒤에 더해진다.

<br>
위에서 구한 **미래 Reward의 가중치 합**을 **Return**이라고 부릅니다. 

Agent의 최종 목표는 이제 **Return 값을 가장 높게 하는 Action을 선택하는 것**이 되었습니다.

<br>

## Markov Decision Process

그럼 다시 본래 주제인 Markov Decision Process로 돌아오도록 하겠습니다. 먼저 앞서 정의했던 3가지 요소를 다시 살펴보도록 하겠습니다.

- 현재 Agent가 놓인 상황
- 다음에 취할 수 있는 Action 목록
- 각 Action에서 얻게 되는 ~~Reward~~ -> Return

첫 번째 항목은 **State**라고 부릅니다. 이는 Agent가 관측 가능한 환경의 범위를 포함합니다. 관측 가능한 모든 영역이 State에 포함되지만 계산의 편의성을 위해 꼭 필요한 부분만으로 한정 짓기도 합니다.

두 번째 항목은 **Action**입니다. State에 따라 Action의 목록은 변화할 수 있습니다. Agent는 자신이 선택 가능한 Action 전체를 알고 있다고 가정합니다.

세 번재 항목은 **Return**입니다. 전체 Reward에 Discounting Factor를 고려한 총합이 되어, 선택의 지표로 활용됩니다. Return을 안다는 것은 미래 state와 action에서 얻게 될 Reward 전체를 파악하고 있다는 의미이기도 합니다.

위 세 가지 요소가 있다면 충분히 Action 선택이 가능할 것으로 보입니다. MDP는 말 그대로 "결정을 내리는 과정" (Decision Process)을 뜻하기 때문입니다. 

하지만 아직 가장 중요한 것이 정해지지 않았습니다. 다음 단계의 Action을 선택하는데 있어서 얼마만큼의 과거 State를 고려할 것인지입니다. 그리고 이 대답은 "Markov"에 담겨 있습니다.

<br>

### Markov

마코프 체인(Markov Chain)은 다음 상태를 예측하기 위해 과거 한 단계 이전만 고려하는 방식을 일컫습니다.

예를 들어, 내일의 날씨를 예측해야 한다면 내일의 한 단계 이전인 오늘의 날씨만을 고려하는 것입니다.

MDP 역시 마찬가지입니다. 현재 선택한 Action으로 갈 수 있는 State는 현재 State에 기반합니다. 그 이전 State는 살펴볼 필요가 없는 것입니다. 현재 단계의 Action 선택은 곧 다음 단계의 State 선택과 동일하며, 한 State 선택은 이전 State만을 고려하면 된다는 것입니다.

<br>

### State Transition Probability (상태 전이 확률)

문제는 다음 State를 위해 선택한 Action이 의도치 않은 결과로 이어질 수 있다는 것입니다.

바둑에서는 내가 돌을 놓고 싶은 곳에 돌을 두지 못하는 경우는 없습니다. 이 경우 한 State에서 다음 state로 전이될 확률은 무조건 "1"이라고 볼 수 있겠죠.

하지만 슈퍼 마리오 게임을 한다고 가정해봅시다. 우리는 장애물을 넘기 위해 점프를 하려 합니다. 그런데 하필 바닥에 얼음이 있어 마리오는 내가 원하는 지점을 벗어난 곳에 도달하고 맙니다. 얼음과 같이 무작위 결과로 이어질 요소가 있다면, 당연히 해당 요소 역시 고려해야 합니다.

그래서 우리는 각 State마다 Agent가 취한 Action이 어떤 State로 이어질지에 대한 확률을 계산합니다. 그리고 이 확률을 **State Transition Probability (상태 전이 확률)**라고 부릅니다.

모든 State에 대해 다음 State로 전이될 확률을 파악하고 있다면 우리가 목표로 하는 State로 이동하게 해줄 Action을 고르는 것이 가능하게 됩니다.

앞서 예를 든 게임 중 바둑처럼 모든 State Transition Probability가 1인 환경을 **Deterministic Environment**라고 부릅니다. 

반대로 슈퍼 마리오처럼 원치 않는 결과가 일어날 수 있는 환경을  **Stochastic Environment**라고 부릅니다.

<br>

### 맺음말

MDP의 구성요소는 결국 다음과 같습니다.

- State
- Action
- Return
- Discounting Factor
- State Transition Probability

이 5가지 요소를 모두 파악한다면 Agent는 항상 최적의 선택이 가능해집니다.

다음에는 실제 Agent가 문제 해결을 위한 최적화 과정을 보며, MDP가 어떻게 최적화에 도움을 줄 수 있는지 살펴보고자 합니다.

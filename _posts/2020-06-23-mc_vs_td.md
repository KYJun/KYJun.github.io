---
layout: post
title: "강화학습 튜토리얼 08 : Monte-Carlo와 Temporal Difference"
categories: rl
--- 
Monte-Carlo vs. Temporal Difference

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
>3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> 피드백은 항상 환영합니다.

### 학습 준비

학습을 진행하기 앞서 시뮬레이션을 통해 우리는 적절한 샘플을 모았습니다.

각 샘플은 훈련을 위한 입력값이면서 동시에 출력값이 됩니다.

이번 포스트에서는 훈련에 샘플을 이용하는 방식에 대해 자세히 살펴보겠습니다.

<br>

### 몬테 카를로 (Monte-Carlo, MC)

시뮬레이션에서 우리는 각 단계(step)의 주요한 정보를 수집합니다.

매 단계마다 상태 정보에 해당하는 State, 현재 단계에서 선택한 Action, Action을 통해 얻게 될 Reward가 주요한 정보가 됩니다.

마지막에 목표를 달성하였는지에 대한 여부 또한 중요한 정보라고 볼 수 있겠죠.

이 달성 여부를 토대로 우리는 모든 단계를 평가할 수 있게 됩니다.

결과가 성공이라면 모든 단계는 긍정적인 점수를 부여하게 되고, 반대로 실패했다면 모든 단계는 부정적인 점수를 부여하게 됩니다.

우리가 수집한 정보는 모든 단계의 정보를 망라하고 있으며, 이러한 샘플의 집합을 Episode라고 부릅니다.

Episode를 가지고 있는 경우, 각 단계마다 전체 Reward의 가중치 합인 Return을 쉽게 구할 수 있게 됩니다.

<br>

![return](/assets/03_mdp/return.png)
> Return을 Discount Factor를 이용해 구하는 공식

<br>

Return을 사용하는 경우 우리의 정책(Policy)은 다음과 같았습니다.

**"Agent가 현재 State에서 Return을 가장 크게 만드는 Action을 선택하는 것"**

즉, 최종 State에 도달했기 때문에 우리는 Return을 활용해 매 단계의 과정을 평가할 수 있게 됩니다.

이런 식으로 Episode 단위의 전체 샘플을 통해 구한 Return으로 각 단계별 State와 Action에 점수를 부여하는 방식을 **몬테 카를로(Monte-Carlo)** 방식이라고 합니다.

몬테 카를로 방식의 샘플 반영 방식은 딥마인드의 알파고가 활용한 방식이기도 합니다.

<br>

![mc](/assets/08_mctd/mc.png)

몬테 카를로 방식의 학습은 다음 순서로 진행됩니다.

1. 결과에 도달할 때까지 시뮬레이션을 진행하며 정보를 저장
2. 결과에 따른 학습 방향을 지정
3. 각 단계마다 Return을 계산
4. Return을 최대화 하는 방향으로 학습 진행 

<br>

### 시간차 학습 (Temporal Difference, TD)

Return과 다른 지표인 Q-Value와 이를 구하는 방법에 대해 지난 포스트에서 다룬 적이 있습니다.

<br>

![bellman](/assets/05_vp/bellman.png)
> Q-Value를 구하는 Bellman Equation

<br>

Q-Value는 전체 샘플이 아닌, 단일 샘플의 State와 Action의 pair에 부여하는 점수입니다.

Q-value를 사용하는 경우 우리의 정책(Policy)은 다음과 같았습니다.

**"Agent가 현재 State에서 가장 Q-value가 큰 Action을 선택하는 것"**

Q-value와 Return의 가장 큰 차이는 필요로 하는 샘플의 개수입니다. 하나의 Episode가 온전히 필요한 Return과 달리 Q-Value는 단일 샘플에 대한 정보만 주어진다면 값을 구할 수 있습니다.

필요한 요소는 현재 State, 선택한 Action, 다음 State의 Reward, 그리고 다음 State의 Q-value의 기댓값입니다.우리가 다음 단계의 Q-value를 안다는 가정 하에서, 우리가 필요한 정보는 현재 단계에만 집중되어 있습니다. 

매 단계별로 차이를 구해 학습을 진행한다는 점에서 해당 방법은 **시간차 학습(Temporal Difference)** 방식이라고 불립니다.

해당 방식의 대표적인 예인 Deep Q Network에 대해선 다음 포스트부터 순차적으로 살펴볼 예정입니다.

<br>

![td](/assets/08_mctd/td.png)

시간차 방식의 학습은 다음 순서로 진행됩니다.

0. 각 단계별 Q-Value 초기값 부여
1. 단계를 한 번 진행하여 State, Action, Reward 정보 수집
2. 현재 단계의 Q-Value를 계산
3. Q-Value를 최대화하는 방향으로 학습 진행

<br>

### 몬테 카를로(MC) vs. 시간차(TD)

MC 방식과 TD 방식은 학습 과정에서 샘플 전체를 한 번에 사용하느냐, 하나씩 사용하느냐의 차이를 갖습니다.

둘의 차이는 지도학습에서 Batch 방식과 Stochastic 방식의 차이와 유사하다고 할 수 있습니다.

샘플 데이터 전체를 활용하는 MC 방식은 모든 입력값을 한 번에 활용하는 Batch 방식과 유사한 특징을 갖습니다. 많은 양의 데이터의 평균을 따라가기에 일부 Outlier가 주는 영향력이 작아 Bias가 낮고 Variance가 높아지게 됩니다.

반대로 샘플 데이터를 한 단계씩 활용하는 TD 방식은 데이터를 하나씩 활용하는 Stochastic 방식과 유사합니다. 소수 샘플에 큰 영향을 받을 수 있어 Bias가 높고 Variance가 낮아집니다.

기계학습에 있어서 Bias와 Variance Trade-off에 대해 자세한 내용은 관련 링크를 참조하겠습니다. ([Bias-Varinace Trade-Off 위키](https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84))

MC 방식은 Return을 활용하며, 최적의 Policy를 바로 수렴하게 하는 방식에 주로 활용됩니다. Policy Gradient 알고리즘이 대표적입니다.

TD 방식은 Q-value를 활용하여, Value를 통한 Greedy 방식의 Policy를 수렴하는 방식에 주로 활용됩니다. Deep Q Network가 대표적인 예시라 볼 수 있습니다.

MC와 TD를 현명하게 활용하기 위해선 시뮬레이션 환경의 특징을 고려하여, 적절한 지표를 선택해야 합니다.

<br>

### 맺음말

MC와 TD라는 데이터를 학습에 활용하는 두 방식에 대해 설명했습니다. 

각각이 학습 과정에 어떻게 반영되는지 등의 보다 자세한 설명은 이후 다루게 될 알고리즘 예시에서 진행하도록 하겠습니다.

우선, 강화학습이 어떻게 인공신경망과 결합할 수 있는지를 다뤄보도록 하겠습니다.

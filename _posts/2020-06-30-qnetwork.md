---
layout: post
title: "강화학습 튜토리얼 09 : Q-Network"
categories: rl
--- 
Q-Network : Q-value와 신경망의 결합

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
>3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> 피드백은 항상 환영합니다.

### Q-Value

이번 포스트에서는 실제로 강화학습을 진행하는 방법에 대해 알아보고자 합니다.

먼저, 가장 쉽고 편하게 다룰 수 있는 Q-Value에 집중해보도록 하겠습니다.

Q-Value를 활용하는 강화학습의 흐름은 다음과 같습니다.

1. 시뮬레이션 : Agent는 자유롭게 환경과 상호작용합니다. 
2. 평가 및 정답 데이터 생성 : 일련의 시뮬레이션에서 선택한 Action과 거쳐온 State에 대해 게임의 결과에 따른 평가를 기록합니다.
3. 훈련 : 매 State별로 Q-value에 대한 훈련을 진행합니다.
4. 반복 : 결과가 수렴할 때까지 1-3 단계를 반복합니다.

Q-Value를 활용하는 경우, 우리의 목표가 되는 Policy는 해당 값이 최대가 되는 Action을 선택하는 **Greedy Policy**가 됩니다.

또한, 시뮬레이션을 통해 얻은 샘플은 매 State마다 값을 업데이트하는 방식인 **Temporal Difference(TD)** 방식을 활용하게 됩니다.

위 두 가지에 대해 추가적인 설명을 원하신다면 지난 포스트를 참고해주시기 바랍니다.

- [시뮬레이션 단계의 Approach 관련](/rl/2020/06/09/approach.html)
- [Temporal Difference 관련](/rl/2020/06/23/mc_vs_td.html)

<br>

### Q-Table

Q-Value는 매 State와 Action의 조합마다 존재할 수 있습니다. Q-Value는 학습이 진행되면서 변화하며, 계속 기록되어야 합니다.

변화하는 값을 관리하는 가장 좋은 방법은 큰 표를 만드는 것입니다.

설명을 위해 게임 하나를 예로 들어보겠습니다.

Frozen Lake 게임은 다음과 같은 특징을 가집니다.

![frozen_lake](/assets/09_qnet/frozen_lake.png)

- Agent는 전체 4x4 크기의 얼어붙은 호수를 건너게 됩니다. 
- S에서 시작하여, G에 도달하면 게임을 클리어할 수 있습니다.
- 함정인 H로 가면 게임 오버가 되므로 안전한 F만을 지나가야 합니다. 
- 바닥이 얼음이기 때문에 Agent는 미끄러져 원치 않는 State로 이동할 가능성이 있습니다.

즉, 우리의 목표는 **"S에서 G로 가는 빠르고 안전한 길을 찾는 것"**이 됩니다.

Frozen Lake의 전체 State는 갈 수 있는 구역 수와 같은 4 * 4 = 16입니다. 각 구역마다 취할 수 있는 Action은 위, 아래, 왼쪽, 오른쪽의 총 4가지입니다.

결과적으로 우리가 구해야 할 Q-Value는 모두 **64가지**가 됩니다.

- 몇 번의 시뮬레이션을 돌리면 모든 가능성을 볼 수 있을까요?
- 언제 Q-Value는 이상적인 값에 수렴했다고 볼 수 있을까요?
- 미끄러져 다른 곳으로 가는 가능성은 어떻게 고려해야 할까요?
- 한 번도 방문한 적 없는 State-Action의 Q-Value는 어떻게 구해야 할까요?

이렇게 단순한 게임을 해결하는 데에도 다양한 고려사항이 필요함을 알 수 있습니다.

하지만 우리는 다시 목표를 살펴보아야 합니다.

우리의 목표는 "어떤 상황에서든 최고의 답을 내놓는 인공지능을 만드는 것"이 아니라 **"가능한 상황에서 최선의 답을 내놓는 인공지능을 만드는 것"**입니다.

목표 달성을 위해 64가지 모두를 고려할 필요도 없고, 완벽한 정답만을 출력하지 않아도 됩니다. 최대한 정답과 유사한 답을 내놓기만 하면 되기 때문이죠.

여기서 **_함수_**가 개입하게 됩니다.

<br>

### Function Approximation

함수는 다항식이며, 입력값과 출력값을 가집니다.

만약 현재 State와 Action을 입력하면 해당 조합의 Q-Value를 출력해주는 함수를 만들 수 있다면 어떨까요? 

64개의 변화하는 값을 갖는 표 하나를 계속 들고 다니는 것보다, 식 하나를 들고 다니는 것이 가볍다는 것은 당연한 얘기일 것입니다.

한 번도 방문한 적 없는 State 혹은 Action이라고 하더라도 함수는 어떤 값을 줄 것입니다. 완전히 터무니 없는 값이 아니라 지난 학습의 결과를 어느 정도 반영한 값이겠죠.

이처럼 함수는 메모리 부담을 덜어주고, 융통성을 갖게 해준다는 장점이 있습니다.

문제는 이런 함수를 만드는 방법입니다.

우리는 학습을 통해 변화하는 함수를 만드는 효율적인 방법을 알고 있습니다.

**_"인공신경망"(Artificial Neural Network, ANN)_**을 활용하는 것입니다.

<br>

### Q-Network

인공신경망은 입력값의 결과가 주어진 출력값과 일치하도록 만들기 위한 기계학습 알고리즘입니다.

인공신경망을 만드는 방법은 이미 지도학습 분야에서 무수히 많이 연구되어왔고, 효율적이고 성공적인 결과는 이미 많이 접해보셨으리라 생각합니다.

강화학습은 이런 지도학습의 방법을 활용해 Q-Value의 학습과 도출을 쉽게 할 수 있게 됩니다. 

인공신경망은 지도학습 방법에 속하기 때문에 입력값과 출력값이 필요합니다. 우리의 목표는 주어진 입력값에 맞는 Q-Value를 출력해주는 함수입니다.

> f(state, action) = Q-Value

위 식이 맞는지 검증하기 위해선 어떻게 해야 할까요?

우리는 Bellman Equation이라는 식으로 Q-Value를 구해왔습니다. 즉, 우리가 원하는 Q-Value 값은 Bellman Equation의 결과값과 동일해야 하죠.

> f(state, action) = reward + discount-factor * E(Q(next-state, next-action))

우리가 구하고자 하는 결과값이 목표로 하는 정답값은 위와 같이 표현할 수 있습니다. 해당 Action을 선택하였을 시에 얻게 되는 reward에 다음 State에 가능한 Q-Value 전체의 기대값을 discount factor를 곱해서 더한 값이죠.

문제는 다음 단계의 Q-Value를 어떻게 구하느냐입니다. 여기서 한 가지 트릭이 들어갑니다.

> **f(state, action) = reward + discount-factor * E(f(next-state, next-action))**

우리에게 주어진 다음 단계의 Q-Value를 구하는 방법은 한 가지입니다. 바로 Network를 사용하는 것이죠. 비록 훈련이 되지 않은 상황에서 초기값으로만 구성된 Network의 결과는 신용할 수 없을 것입니다.

하지만 Q-Network의 파라미터는 꾸준히 갱신될 것이며, 동시에 우변에 들어가는 다음 단계의 Q-Value 값도 같이 개선될 것입니다. 결과적으로 훈련을 계속하면 하나의 값에 수렴할 수 있다는 것이죠.

여기서 강화학습의 딥러닝이 지도학습의 딥러닝과의 차이점으로, 학습 목표에 해당하는 정답 데이터가 네트워크의 훈련에 따라 시시각각 변화한다는 점을 알 수 있을 것입니다.

이러한 차이는 당연하게도 Q-Network 학습의 단점으로 지적되었습니다. 문제의 자세한 요지와 이를 극복하기 위한 대안은 다음 포스트에서 살펴보도록 하겠습니다.

<br>

### 훈련 절차

정리하면, Q-Network는 다음과 같은 절차로 학습이 진행됩니다.

1. 시뮬레이션을 통한 샘플 생성
2. Network를 통해 얻은 Q-Value와 실제 샘플에서 Bellman Equation을 통해 계산한 Q-Value의 차이를 계산
3. 해당 값을 Loss로 하여, 역전파법을 통해 변수 갱신
4. 1-3을 반복하여 모델 변수를 수렴

<br>

### 맺음말

Q-Network는 지도학습의 인공신경망을 따왔지만, 자체적 한계로 인해 타협을 하게 되었습니다.

타협은 문제로 이어지고, Q-Network는 구글의 DeepMind가 Deep Q Network를 발표하기 전까지는 크게 각광받지 못하였습니다.

다음 포스트에서는 본격적인 강화학습 딥러닝 시대를 연 Deep Q Network에 대해 다루겠습니다.

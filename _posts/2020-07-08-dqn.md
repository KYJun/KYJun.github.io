---
layout: post
title: "강화학습 튜토리얼 10 : Deep Q-Network"
categories: rl
--- 
Deep Q-Network (DQN)

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
>3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> 피드백은 항상 환영합니다.

### Q-Network의 한계

지난 포스트에서는 Q-Value를 최적화할 때, 인공신경망을 훈련시키는 방법을 접목시킨 Q-Network에 대해 다뤘습니다.

하지만 기존 강화학습 알고리즘과 다른 인공지능을 활용했다는 점에 주목받았지만 초기의 Q-Network는 좋은 성과를 보이지 못했습니다. 

여기에는 다음 두 가지 문제가 있었습니다.


#### 1. 안정성 문제 

인공신경망은 하나의 커다란 함수와 같습니다. 방문한 적 없는 State에 대해서도 유사값을 구할 수 있다는 점은 신경망의 높은 유연성을 보여줍니다. 

**하지만 융통성이 큰 방법은 안정적이지 못하다는 문제가 있습니다.** 

네트워크 방식에서도 여전히 Q-Value는 Bellman Equation을 활용하여 계산됩니다.

훈련의 목표를 계산하기 위해 다시 훈련의 입력값을 사용한다는 불안정성은 파라미터 갱신을 통해 얻은 출력값의 신뢰도를 떨어뜨리게 만듭니다.

또한, 학습을 위해 주어지는 데이터가 쉽게 정제 가능한 지도학습과는 달리, 인간에 의해 100% 통제되기 힘든 환경과의 상호작용을 거쳐 나오는 점도 개입하게 됩니다.

#### 2. 데이터 효율성 문제

Q-Value는 Temporal Difference (TD) 방식으로 구해집니다. Q-Network에서도 마찬가지로, 매 학습마다 하나의 샘플이 활용되게 됩니다.

이는 지도학습에서 한 샘플씩 훈련을 진행하는 Stochastic Gradient Descent (SGD)를 연상케 합니다.

그러나 TD와 SGD 사이의 가장 큰 차이점은 데이터의 재활용에 있습니다.

지도학습은 동일한 데이터를 반복해서 활용 가능합니다. 전체 데이터가 한 번씩 변수 갱신에 활용되면 다시 처음으로 돌아가 같은 작업을 반복할 수 있습니다.

**하지만 강화학습은 동일한 데이터를 사용할 수 없습니다.** 

특정 State에서 한 Action을 고른 샘플을 훈련에 사용합니다. 한 번 변수를 갱신하는 과정에 의해 Q-Value는 훈련 이전과 이후가 완전히 변화하게 됩니다.

안정성 문제와 마찬가지로, 훈련의 결과이면서 입력값인 Q-Value는 데이터 재활용을 불가능하게 만들고 훈련마다 시뮬레이션을 돌려야 하는 비효율적인 데이터 활용 문제를 야기시킵니다.

<br>

### Deep Q Network

구글의 DeepMind는 2015년, Q-Network를 재탄생시킵니다.

> Deep Q Network 논문 링크 ([arxiv](https://arxiv.org/abs/1312.5602))

Deep Q Network이란 이름의 이 네트워크는 놀랍게도 벽돌 깨기를 비롯한 아타리 사의 비디오 게임을 사람 이상의 수준으로 클리어하게 됩니다.

DQN은 어떻게 이런 결과를 만들어낼 수 있었을까요?

딥마인드는 논문에서 3가지 방법을 제시합니다.

1. Convolution Network 
2. Experience Replay
3. Target Network

![dqn](/assets/10_dqn/dqn.png)

1번은 기존 인공신경망 부분을 합성곱신경망(CNN)으로 대체했다는 것을 가리킵니다. 

CNN은 이미지 분류에 큰 성과를 보였기 때문에 게임 화면의 특징을 추출하는 데 활용하기에 더할 나위 없었을 것입니다. 

CNN은 지도학습의 방식에 더 가까워 해당 포스트에는 다루지 않고, 더 자세히 알고 싶으신 분을 위해 관련 설명이 적힌 링크를 남기겠습니다.

본 포스트에서는 2번과 3번을 보다 집중적으로 다뤄보고자 합니다. 앞서 언급한 문제의 해결책을 제시해주었기 때문입니다.

<br>

### Experience Replay

DQN이 기존 Q-Network와 다른 점은 시뮬레이션 단계에서 샘플을 저장하는 방법에 있습니다. 

기존 Q-Network에서 지적받은 데이터 효율성 문제를 해결하기 위해서, DQN은 Experience Replay 방식을 활용합니다.

먼저 시뮬레이션을 통해 얻은 샘플을 Buffer에 저장합니다. Buffer는 항상 일정한 개수의 샘플을 보유하며, 새로운 샘플이 추가되면 가장 오래된 샘플을 삭제합니다.

훈련에서는 Buffer에서 미리 정한 개수만큼의 샘플을 무작위로 추출하여 진행합니다.

사전에 정한 샘플 개수를 Batch Size라고 보면, 해당 훈련 방식은 지도학습에서의 미니 배치 (Mini batch) 훈련 방식과 유사하다고 볼 수 있습니다.

해당 방식의 장점은 우선 샘플을 재활용할 수 있게 됨으로써, 효율적인 데이터 관리가 가능해졌습니다.

또한, 항상 시간 순서대로 파라미터 갱신을 해야 했던 기존 방식과 달리, 시간 순서에 영향을 받지 않는 일반적인 모델 훈련이 가능해졌습니다.

<br>

### Target Network

DQN의 세 번째 특징은 Target Q-network를 별도로 구축했다는 점입니다.

DQN은 시뮬레이션 단계에서 샘플을 뽑는데 활용되는 Q-Network와 훈련 단계에서 다음 단계의 Q-Value를 계산할 때 활용되는 Q-Network를 분리하였습니다.

두 네트워크는 같은 모습을 띄고 있고, 후자를 Target Network라고 부릅니다.

Target Network의 가장 큰 특징은 변수 갱신이 더디게 이뤄진다는 점에 있습니다.

시뮬레이션에 사용하는 Q-Network에선 지도학습에서의 변수 갱신처럼 Back Propagation의 Gradient 값을 그대로 반영하지만, Target Network에서는 값의 일부만 변수 갱신에 반영합니다.

이런 반영 방법은 앞서 언급했던 네트워크의 불안정성 문제를 해결할 수 있습니다. 

전체를 반영하지 않음으로써 훈련 과정에서 Q-Network가 요동치지 않게 하고, 천천히 그러나 안정적으로 이상적인 값에 수렴할 수 있도록 하는 것입니다.

또한, 시뮬레이션과 훈련 단계에서 다른 Q-Network를 사용했다는 점은 다른 Policy를 활용했다고도 볼 수 있습니다.

이렇게 두 Policy를 구분하는 방식을 Off-Policy 방식이라 부른다고 지난 포스트에서 다뤘습니다.

Off-Policy 방식의 적용을 가로막던 복잡한 계산 없이 단순히 네트워크 두 개를 사용하는 것으로 목적을 쉽게 달성할 수 있게 된 것입니다.

<br>

### 훈련 방식

DQN의 훈련 방식은 정리하자면 다음과 같습니다.

0. 동일한 설정으로 두 개의 Q-Network를 작성 후, 초기 값 부여
1. 시뮬레이션을 통해 얻은 샘플을 Buffer에 저장
2. Buffer에서 무작위로 샘플을 추출하여 배치(batch) 생성
3. 배치를 Target Network를 거쳐 Q-Value 값을 얻고, Bellman Equation을 통해 계산한 값과의 차이를 계산
4. 해당 값을 Loss로 하여, 역전파법을 통해 Gradient 값을 계산
4. 시뮬레이션 네트워크는 Gradient를 전체 반영
5. Target Network는 Gradient를 정해진 비율에 따라 일부만 반영
6. 모델의 각 변수가 일정 값에 수렴할 때까지 1-5를 반복

<br>

### 맺음말

DQN은 강화학습과 인공신경망이 결합된 성공적인 사례로 크게 주목 받았고, 성능 개선을 위한 후속 연구 역시 꾸준히 지속되었습니다.

다음 포스트에서는 이러한 후속 연구 중 큰 차이를 불러온 몇 가지를 같이 살펴보도록 하겠습니다.

---
layout: post
title: "강화학습 튜토리얼 01: 강화학습 소개"
categories: rl
---
Introduction : 강화학습(RL) 개념과 목적 설명

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
> 3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> - 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> - 피드백은 항상 환영합니다.
> - 모바일에서는 읽기 모드로 바꿔서 읽는 걸 추천드립니다.

### 지도학습, 비지도학습, 그리고 강화학습

기계학습을 분류할 때, 흔히 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)으로 나누곤 합니다. 

![기계학습 분류](https://miro.medium.com/max/1400/0*F23wacf0xFsV4I40.jpg)
> (이미지 출처 : bit.ly/2ONAKQg)

<span style="color:red">**지도학습**</span>은 특정 입력값이 주어지면 원하는 출력값이 나오도록 모델을 학습하는 방법을 가리킵니다. 

따라서, 내가 원하는 데이터의 입력값과 출력값 모두가 완벽하게 준비되어 있어야 합니다.

이미지 분류, 텍스트 생성 등 현재 인공신경망을 활용하는 기계학습의 거의 대부분은 지도학습 방식을 따르고 있습니다.

<br/>

<span style="color:green">**비지도학습**</span>은 주어진 데이터의 특징을 분석하는 방법입니다.

지도학습과 달리 목표가 되는 정답은 주어지지 않으며, 단지 데이터 내에 존재하는 패턴 혹은 분포를 찾습니다.

비슷한 특징을 갖는 데이터를 묶어서 군집화를 하는 경우가 대표적인 비지도학습의 예시라 할 수 있습니다.

<br/>

<span style="color:orange">**강화학습**</span>은 지도학습도, 비지도학습도 아닌 제 3의 방법으로 분류됩니다. 

<br>

강화학습이 앞선 두 방식과 완전히 다른 것이 아니라, 두 방법의 특징을 모두 갖고 있기 때문입니다.

- 강화학습에 사용하는 데이터는 비지도학습과 마찬가지로 정답지가 없습니다.

- 강화학습의 학습 단계에서는 지도학습과 같은 방법이 적용됩니다.

정답이 없는 데이터에서, 정답을 따르는 학습을 실행하기 위해 강화학습은 추가적인 단계를 밟게 됩니다.

강화학습이 이뤄지는 과정 이해를 위해 먼저 강화학습이 달성하고자 하는 목표를 살펴볼 필요가 있습니다.

<br/>

### 강화학습의 목적

강화학습이라는 단어를 들으면, 알파고(AlphaGo)가 먼저 떠오르실 겁니다.

구글의 딥마인드(Deepmind)는 강화학습을 통해 아타리(Atari)의 비디오 게임, 바둑에 이어, 최근에는 스타크래프트2까지 정복해 나가고 있습니다.


![alphago](https://miro.medium.com/max/1400/0*mK7a7fkAl1OVviL0.)
> 강화학습은 게임에 집중하고 있다. 
<br/>
> (이미지 출처 : https://bit.ly/2WGQzdt)

강화학습의 목표는 **최선의 선택**을 하는 것입니다. 특정한 상황에서 강화학습은 어떤 행동이 가장 좋은 결과로 이어질 것인지를 판별합니다. 

<br/>

최적의 선택은 어떻게 가능할까요? 


- 먼저 현재 상황을 확실히 분석해야 합니다. 상황에 대한 분석은 다양한 지식이 요구되며, 여기에는 다음에 어떤 행동을 취할 수 있는지에 대한 지식도 포함됩니다. 

- 다음으로 목표를 확실히 해야 합니다. 선택과 행동은 최종적으로 목표 달성을 위한 것입니다. 목표를 정확하게 정의하여, 지금의 상황과 행동이 목표에 도움이 되었는지 아닌지를 평가할 수 있어야 할 것입니다.

<br>

**_강화학습은 현실의 문제를 위한 기계학습 방식입니다._**

우리는 현실에서 다양한 상황에 맞닥뜨리게 되고, 그때마다 선택의 기로에 놓이게 됩니다. 

![choice](https://cdn.pixabay.com/photo/2017/05/17/07/29/direction-2320124_1280.jpg)

강화학습의 지향점은 이런 선택을 대신해주는 인공지능을 만드는 것입니다. 

하지만 강화학습을 현실에 적용하기엔 넘어야 할 벽이 아직 너무나 많습니다. 

우리가 당연히 여기고 행하는 일들 속에 숨겨진 모든 물리 법칙과 배경 지식들을 전부 감안하고, 여러 목표들 중 가장 만족스러운 선택을 고르는 일은 불가능에 가깝죠.

그래서 대신 현실을 닮았지만 현실보다 단순한 **게임** 속 세계에서 선택을 연습하게 됩니다.

<br/>

### 게임과 시뮬레이션

게임 속 세상이라고 하지만 여전히 마주할 수 있는 상황은 무수히 많습니다. 

이런 무수히 많은 상황마다 올바른 선택을 일일이 알려주는 건 사람에게나 기계에게나 피곤한 일이죠. 특히, 올바른 선택이 하나 이상일 경우에는 더욱 피곤해질 것입니다. 

결국 선택의 옳고 그름은 **결과**에 달려 있기 때문입니다.

그래서 우리는 우선 기계에게 선택을 맡깁니다.기계가 스스로 판단하여 동작하도록 말이죠.

물론 처음부터 기계가 게임을 깨지는 못할 겁니다.

우리는 기계에게 자신의 선택이 틀렸음을 알려주고, 다시 처음부터 선택하도록 합니다.

이런 과정을 반복하다 보면 기계는 스스로 선택을 바꿔가면서 게임을 정복해 나갈 것입니다.

이것이 앞서 언급한 **시뮬레이션** 단계이며, 학습을 위한 정답지를 수집하는 과정이 됩니다.

강화학습은 시행착오를 반복하며 스스로를 강화시키는 학습방법인 것입니다.

### 맺음말

강화학습의 키워드는 **선택**과 **시행착오**입니다. 

앞으로의 포스트에서는 강화학습의 구성요소와 더불어 어떻게 선택을 해나가고, 어떻게 시행착오를 효율적으로 진행하게 할 수 있을지에 대한 이론을 살펴보고자 합니다. 
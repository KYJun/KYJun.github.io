---
layout: post
title: "강화학습 튜토리얼 06 : Exploration vs. Exploitation"
categories: rl
---
Random, Greedy 그리고 Epsilon-Greedy 

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
>3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> 피드백은 항상 환영합니다.

### 강화학습의 흐름

이전 포스트에서는 강화학습의 최적화 대상으로써 Model과 Value, Policy를 살펴보았습니다. 

이후 살펴볼 강화학습은 Model-Free를 기반으로 하기 때문에 시뮬레이션 단계는 필수 불가결합니다.

Model-Free 강화학습의 큰 흐름은 다음과 같습니다.

1. 시뮬레이션 : Agent는 자유롭게 환경과 상호작용합니다. 
2. 평가 및 정답 데이터 생성 : 일련의 시뮬레이션에서 선택한 Action과 거쳐온 State에 대해 게임의 결과에 따른 평가를 기록합니다.
3. 훈련 : 맞은 선택이라면 권장하는 방향으로, 틀린 선택이라면 지양하는 방향으로 Value 혹은 Policy에 대한 훈련을 진행합니다.
4. 반복 : 결과가 수렴할 때까지 1-3 단계를 반복합니다.

이번 포스트에서는 시뮬레이션 단계를 어떻게 진행하는지 자세히 살펴보고자 합니다.

<br>

### 시뮬레이션

시뮬레이션 단계는 강화학습의 입력값과 출력값을 모두 만들어내는 단계이기 때문에 무척 중요합니다.

시뮬레이션을 어떻게 진행하느냐에 따라 강화학습은 빠르게 정답을 찾을 수도 있고, 반대로 길을 잃고 계속 헤매게 될 수도 있습니다.

적절한 시뮬레이션을 위해선 그러나 해결해야 할 딜레마가 존재합니다.

한 가지 예를 들어 봅시다.

미로에서 A와 B는 갈림길에 마주하게 되었습니다. 이전에 같은 갈림길에서 둘은 왼쪽 길을 택했고, 출구에 도착할 수 있었습니다. 

이번에는 어느 길을 선택해야 할까요?

A는 왼쪽 길을 선택했습니다. 이전에 왼쪽 길로 출구에 도착했기 때문에 합리적인 판단이라고 주장합니다.

B는 오른쪽 길을 선택했습니다. 한 번의 경험만으로 왼쪽 길이 정답이라 하는 것은 잘못된 생각이라고 주장합니다.

누구의 선택이 옳은 걸까요?

답은 "결과"로 알 수 밖에 없습니다.

<br>

### Exploration vs. Exploitation

Model-Free 상황에서 시뮬레이션은 답을 알지 못하는 상황에서 진행됩니다.

그런 상황에서 시뮬레이션 상의 선택은 신중해집니다.

A는 안전한 길을 택했습니다. 과거의 경험을 기반으로 가장 큰 이득을 얻을 수 있는 선택을 한 것입니다. 

이처럼 이전 선택을 기반으로 최대한 이득을 얻고자 하는 방식을 **Exploitation**이라고 부릅니다. 배경지식을 활용해 안정성을 택한 것이라 볼 수 있겠죠.

Exploitation에서는 매순간 한정적인 정보 내에서 탐욕적으로 이득을 추구하는 **Greedy Approach**를 선호하게 됩니다.

다음으로, B는 무작위로 길을 택했습니다. 과거의 경험은 무시하고 다양한 가능성에 시험해본 것이라 할 수 있습니다.

이처럼 과거 선택과 상관없이, 또는 과거 선택과 상반된 선택을 하는 방식을 **Exploration**이라고 부릅니다. 다양한 가능성을 탐험한다고 볼 수 있겠죠.

Exploration에서는 매순간 가능한 선택지 내에서 무작위로 선택하는 **Random Approach**를 선호하게 됩니다.

Exploitation과 Exploration은 이처럼 서로 상반된 성격을 가졌지만 둘 다 무시할 수 없습니다.

그렇다면 어떻게 둘 사이의 딜레마를 해결할 수 있을까요?

<br>

### Epsilon-Greedy Approach

우선 아무 경험도 없는 초기 상태에서는 Exploitation 기반 선택은 불가능합니다. 

처음에는 무조건 Exploration을 할 수 밖에 없죠.

하지만 시뮬레이션을 반복하여 다양한 경험을 쌓게 되면 더 이상의 Exploration이 의미가 없어지는 순간이 옵니다.

이미 충분히 경험을 쌓았고, 경험을 기반으로 더 좋은 선택을 고르는 Exploitation이 결국 Agent의 최종 목표에 부합하기 때문이죠.

즉, 학습 초기에는 Exploration으로 시작하여 서서히 Exploitation으로 넘어가는 방식이 적합하다고 볼 수 있습니다. 

이런 방식의 전환을 쉽고 효율적으로 해주는 방식이 바로 **"Epsilon-Greedy Approach"**입니다.

Epsilon은 0과 1 사이의 값을 가지며, 특정 시뮬레이션에서 무작위로 선택을 진행할 확률을 나타냅니다.

Epsilon은 처음에 1의 값을 가집니다. 즉, 첫 시뮬레이션은 무조건 Random Approach로 진행되는 것이죠.

하지만 Epsilon은 학습이 진행되면서 서서히 감소하게 됩니다. 

학습이 진행될수록 경험을 살린 Greedy Approach로 시뮬레이션을 진행하게 되는 것이죠.

Epsilon-Greedy Approach를 통해 우리는 Exploration과 Exploitation의 균형을 맞추고, 가장 이상적인 방식으로 시뮬레이션을 진행할 수 있게 됩니다.

<br>

### 해결?

그렇다면 Epsilon-Greedy Approach가 만능 해결법이 될까요?

불행히도 그렇지 않습니다.

문제는 Epsilon이 0이 되지 않는다는 점에 있습니다.

일반적으로 Epsilon의 최솟값은 0.1 혹은 0.01로 정해져 있고, 그 이하로 줄어들지 않게 됩니다.

아주 낮은 확률이지만 시뮬레이션 단계에서 Agent는 무작위 선택을 하게 될 수 있다는 의미죠.

여기에는 Model-Free의 특징이 반영되어 있습니다. 환경을 완전히 알지 못하는 상황에서, 제아무리 많은 시뮬레이션을 돌린다고 하더라도 현재 최적의 길이 완벽한 정답이라고 볼 수 없다는 것입니다.

시뮬레이션은 일종의 모집단 내의 표본집단에 불과하기 때문에, 완전히 전체를 대변한다고 보기 힘들다는 것입니다.

그러나 무작위 선택의 가능성이 열려 있다는 것은 결국 Agent가 자신의 선택을 완전히 통제하지 못한다는 의미가 됩니다.

우리의 목표는 표본을 잘 얻는 Agent가 아니라, 문제를 잘 해결하는 Agent를 구하는 것이기 때문이죠.

해당 문제를 극복하는 방법에 대해서는 다음 포스트에서 다루도록 하겠습니다. 

<br>

### 맺음말

시뮬레이션 단계에서의 Exploitation vs. Exploration 딜레마와 해결책 중 하나인 Epsilon-Greedy Approach에 대해 살펴 보았습니다.

다음 포스트에서는 On-Policy와 Off-Policy를 통해 위에서 설명한 문제를 해결하는 방법을 알아보도록 하겠습니다.

---
layout: post
title: "강화학습 튜토리얼 01: 강화학습 소개"
categories: rl
---
Introduction : 강화학습(RL) 개념과 목적 설명

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
> 3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> - 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> - 피드백은 항상 환영합니다.
> - 모바일에서는 읽기 모드로 바꿔서 읽는 걸 추천드립니다.

### 지도학습, 비지도학습, 그리고 강화학습

기계학습의 방법은 크게 지도학습(Supervised Learning)과 비지도학습(Unsupervised Learning)으로 나눌 수 있습니다. 두 학습 방식은 학습의 목표와 데이터의 종류에 따라 구분할 수 있습니다. 

![기계학습 분류](https://miro.medium.com/max/1400/0*F23wacf0xFsV4I40.jpg)
> (이미지 출처 : bit.ly/2ONAKQg)

<span style="color:red">**지도학습**</span>의 목표는 주어진 입력값과 출력값을 연결하는 것입니다. 어떤 입력값이 주어졌을 때, 좋은 지도학습 모델은 우리가 원하는 출력값을 도출해내게 됩니다. 지도학습을 위해서는 각각의 입력값에 이에 상응하는 출력값이 반드시 매칭되어 있어야 합니다.

스팸 메일 분류와 같은 분류 모델(Classification Model) , 텍스트를 생성해내는 GPT와 같은 생성 모델(Generation Model) 등 현재 인공신경망(Artificial Neural Network, ANN)을 활용하는 기계학습의 거의 대부분은 지도학습 방식을 따르고 있습니다.

<br/>

<span style="color:green">**비지도학습**</span>의 목표는 주어진 데이터의 특징을 파악하는 것입니다.

지도학습과는 달리 비지도학습은 입력값과 출력값이 따로 구분되어 있지 않거나, 구분하기 힘든 데이터를 활용합니다. 비지도학습을 거친 모델은 주어진 데이터의 관계 혹은 분포를 알 수 있게 해줍니다. 

비슷한 특징을 갖는 데이터를 묶어주는 군집화(Clustering)가 대표적인 비지도학습의 예시라 할 수 있습니다.

<br/>

<span style="color:orange">**강화학습**</span>은 지도학습과 비지도학습의 특징을 모두 가지고 있어, 제 3의 학습 방법으로 분류됩니다.

강화학습에서 사용하는 데이터는 입력값으로만 구성되어 있습니다. 이는 출력값, 즉 정답을 학습 단계 이전에 별도로 준비해야 하는 지도학습과는 다른 비지도학습의 특징을 가진다고 볼 수 있습니다.

하지만 강화학습의 훈련 방식은 주어진 입력값에 상응하는 출력값을 도출하도록 진행됩니다. 이는 지도학습과 동일한 방식으로 훈련이 진행된다고 볼 수 있습니다. 

정답이 없는 데이터를 어떻게 정답이 필요한 지도학습 방법으로 훈련할 수 있을까요?

이를 위해선 강화학습이 필요한 상황과 강화학습을 통해 달성하려는 목표가 무엇인지를 확인할 필요가 있습니다.

<br/>

### 강화학습의 목적

강화학습은 AlphaGo를 비롯하여 주로 게임과 연관되는 경우가 많습니다. 왜 게임에서 이기기 위한 인공지능을 만드는데 강화학습 방법이 필요한 걸까요?

![alphago](https://miro.medium.com/max/1400/0*mK7a7fkAl1OVviL0.)
> 강화학습은 게임에 집중하고 있다. 
<br/>
> (이미지 출처 : https://bit.ly/2WGQzdt)

우선, 게임 상황에 대한 고려가 필요합니다. 바둑을 하는 상황이라고 가정해봅시다. 우리가 첫 수를 둘 수 있는 곳은 무수히 다양합니다. 또한, 첫 수 이후에 다음 수를 놓는 곳 역시 다양하며, 어떤 위치에 어떤 순서로 두느냐에 따라 승리 확률은 시시각각으로 변할 것입니다. 또한, 바둑에선 상대의 수를 읽는 것 또한 중요합니다. 상대가 어디에 두었는지에 대한 기록 뿐만 아니라 어디에 둘 것인지에 대한 예측도 필요할 것입니다.

이러한 각각의 상황에 대해서 우리는 모든 입력값과 출력값을 준비할 수 있을까요? 만약 준비하는 과정에서 한 가지 상황이라도 놓친다면 혹은 잘못된 데이터를 입력한다면 우리가 원하는 인공지능을 만들지 못할 것입니다.

설령 모든 가능성을 염두에 둔다고 하더라도 우리가 원하는 속도로 처리할 수 있을까요? 만약 모든 경우의 수를 알고 분석할 수 있다면 당연히 승리할 수 있을 것입니다. 그러나 그 과정에서 터무니없는 시간과 컴퓨터 메모리가 필요하다면 문제가 발생합니다. 모든 경우의 수를 계산하는 것은 불필요하며 이길 수 있는 경우만을 한정하여 빠르고 효율적으로 다음 수를 두는 것이 중요합니다.

강화학습의 목표는 **최선의 선택**을 하게 만드는 것입니다. 현재 주어진 상황에서 어떤 선택이 가장 최적의 결과로 이어질 것인지를 빠르고 효율적으로 도출해내는 것이 강화학습의 목적이 되는 것입니다.

<br/>

최적의 선택은 어떻게 가능할까요? 

- 먼저 현재 상황에 대해 파악해야 합니다. 현재 상황에서 그 동안 어떤 선택을 해왔고, 앞으로 어떤 선택이 가능한지에 대해 이해해야 합니다. 여태까지 어떤 선택을 해왔고, 해당 선택이 주는 피드백을 통해 다음 선택이 줄 수 있는 이득을 예측할 수도 있을 것입니다.

- 다음으로 목표를 확실히 해야 합니다. 게임 상황이라면 승리를 위한 조건을 파악해야 할 것입니다. 특정 점수를 달성하면 되는 것인지, 특정 장소에 도달해야 하는 것인지 혹은 상대를 쓰러뜨려야 하는 것인지. 어떤 조건이냐에 따라 최적의 선택은 바뀔 것입니다.

게임에서 벌어지는 각각의 상황에서 기계는 최적의 선택을 행할 것입니다. 우리는 그 선택에 점수를 매길 수 있습니다. 기계가 행하는 **시뮬레이션** 단계에서, 기계는 스스로 학습을 위한 정답지를 수집하게 되는 것입니다.

강화학습에서는 현재 상황과 가능한 선택이 입력값이 되며, 해당 입력이 목표에 가까워지는지의 여부가 출력값이 됩니다. 우리의 목표는 최적의 출력값을 목표로 하는 입력값을 마련하는 것이 됩니다.

<br>

**_강화학습은 현실의 문제를 위한 기계학습 방식입니다._**

게임은 우리에게 한정된 상황과 선택지를 제공합니다. 달성해야 하는 목표 역시 뚜렷합니다. 그렇기에 강화학습에 필요한 입력값과 출력값을 구하기 쉽습니다. 하지만 강화학습을 통해 이루고자 하는 것은 단순히 게임을 잘하는 인공지능을 만드는 것이 아닙니다.

우리는 현실에서 다양한 상황에 맞닥뜨리게 되고, 그때마다 선택의 기로에 놓이게 됩니다. 

![choice](https://cdn.pixabay.com/photo/2017/05/17/07/29/direction-2320124_1280.jpg)

강화학습의 지향점은 이런 선택을 대신해주는 인공지능을 만드는 것입니다. 

하지만 강화학습을 현실에 적용하기엔 넘어야 할 벽이 아직 너무나 많습니다. 

우리가 당연히 여기고 행하는 일들 속에 숨겨진 모든 물리 법칙과 배경 지식들을 전부 감안해야 하며, '승리'와 같은 명시적인 목표가 없는 경우에 특정 선택이 가져다 주는 이득은 수치화하기 어렵습니다. 이러한 가변적인 상황에서 완벽한 선택을 이끌어 내는 것은 불가능에 가까울 지도 모릅니다. 

그러나 인공지능의 가능성은 간과할 것이 되지 않습니다. 지금은 불가능해 보이더라도 언젠가는 현실의 다양한 부분에서 활약을 하는 강화학습 인공지능을 볼 수 있게 될 것입니다. 몇 번의 시행착오를 거치고, 올바른 선택을 하는 인공지능을 말이죠.

<br/>

### 맺음말

강화학습의 키워드는 **선택**과 **시행착오**입니다. 

앞으로의 포스트에서는 강화학습의 구성요소와 더불어 어떻게 선택을 해나가고, 어떻게 시행착오를 효율적으로 진행하게 할 수 있을지에 대한 이론을 살펴보고자 합니다. 
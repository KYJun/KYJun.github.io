---
layout: post
title: "강화학습 튜토리얼 04 : 모델 (Model)"
categories: rl
---
Model-based vs. Model-free

> 강화학습 튜토리얼은 다음 자료를 참고하였습니다.
> 1. [David Silver RL Lectures](https://www.davidsilver.uk/teaching/)
> 2. [Dennybritz RL Github](https://github.com/dennybritz/reinforcement-learning)
>3. [Arthur Juliani's Medium Posts](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
>
> 최소한의 수식을 사용하고, 누구나 이해할 수 있는 설명을 목표로 작성했습니다.
> 피드백은 항상 환영합니다.

### 학습 목표
성공적인 기계학습을 위해서는 기계가 무엇을 배우게 할 것인지를 구체적으로 정하는 과정이 중요합니다.

지도학습(Supervised Learning)의 경우, 목표는 간단합니다. 정답으로 주어진 데이터를 따라가면 되죠. 

하지만 강화학습은 정답이 바로 주어지지 않는다고 했습니다. 별도의 시뮬레이션 단계를 거쳐 얻은 결과를 통해 학습이 진행됩니다. 각 결과에 대한 평가가 진행되면 Agent는 과거의 경험을 통해 학습하게 됩니다.

이전 포스트에서 저희의 목표는 **Agent가 Return 값을 가장 높게 하는 Action을 선택하는 것**이라고 하였습니다. 

학습이 완료되면 각 State마다 적절한 Action이 지정될 것입니다. 다시 말해, 우리는 현재 State를 입력하면 최적의 Action이 도출되는 **'Model' (모델)**을 얻게 되는 것입니다.

<br>

### Model

![maze](/assets/04_model/maze.png)

미로 찾기 게임을 한다고 가정하겠습니다. 가장 큰 Return을 얻기 위해선 당연히 출구에 도달해야겠죠. 

그렇다면 현재 우리의 목표는 출구에 가장 빨리 도착하게 하는 Action을 선택하는 것입니다.

그런데 우리에겐 마침 입구와 출구가 표시된 미로의 "지도"가 있습니다. 

섣불리 길을 나서기 전에 지도를 보며 최단 경로를 파악하고, 단번에 출구까지 나아갈 수 있을 것입니다.

지도는 저번 포스트에서 살펴본 **MDP**라고 할 수 있습니다. 

우리가 방문 가능한 State, Action, Return을 모두 보여주고 있기 때문이죠. 모든 가능성을 염두에 둘 수 있기 때문에 단번에 최적의 답을 찾을 수 있는 것입니다.

MDP의 모든 구성요소를 안다면 우리는 미로를 해체할 수 있습니다. 불필요한 부분은 없애고, 경로가 될 수 있을 부분만 남겨 새로 미로를 재구성할 수 있죠.

이렇게 재구성된 미로가 바로 **Model**입니다.

MDP의 모든 구성요소를 파악하면 우리는 환경을 기반으로 Model을 구축할 수 있고, 다시 그 Model을 기반으로 목표를 달성할 수 있게 됩니다.

Model은 말하자면 환경의 축소판과 같습니다. 환경과 같은 요소를 갖추고 있지만 MDP를 고려하여, Agent에 도움이 되는 방향으로 재구축되었다고 볼 수 있죠. 

따라서, Agent는 이제 환경이 아니라 Model과 상호작용을 하며 최단 경로를 구해낼 수 있게 됩니다.

Model을 가정하면 우리의 목표는 다음과 같습니다.

**'Agent는 현재 State에서 Model이 제시하는 Action을 선택한다.'**

![maze_model](/assets/04_model/maze_model.png)
> 앞선 미로의 Model

<br>

### 항상 쉬운 길만 있는 것은 아니다
그러나 우리는 미로에 들어갈 때 항상 지도를 받고 시작하지 않습니다. 미로에 대한 외부적인 정보 없이 오직 경험과 시행착오만을 활용해야 할 것입니다.

물론 미로를 이잡듯이 돌아다닌다면 지도를 스스로 만들 수도 있을 겁니다. MDP의 구성요소 전체를 시행착오를 통해 파악하는 것이죠. 

하지만 우리는 미로를 다닐 때 지도를 만드는 것을 우선하지 않습니다. 

우리의 목표는 출구에 도착하는 것이지 미로를 완전히 파악하는 것이 아니기 때문입니다.

실제로 같은 미로를 두 번, 세 번 도전하게 되면 지도 없이도 빠르게 출구를 찾을 수 있을 것입니다.

모든 State와 Action을 고려하기 전에 이미 최적의 길을 찾아냈기 때문이죠.

이렇게 Model의 도움 없이 시행착오를 거쳐 답을 찾는 과정을 **Model-Free** 방식이라고 부릅니다. 

<br>

### Model-Based vs. Model-Free

환경의 일부를 따온 Model을 만들어 문제를 해결하는 Model-Based와 환경과 직접 시행착오를 거쳐 문제를 해결하는 Model-Free 방식을 알아보았습니다.

Model-based와 Model-Free 방식 간 선택에 있어서는 신중할 필요가 있습니다.

Model이 있다면 모든 경우의 수를 고려한 확실한 해답을 알아낼 수 있습니다. 하지만 Model의 복잡도에 따라 더 큰 메모리와 시간적 부담이 발생할 수 있습니다.

Model이 없다면 시행착오만으로 도출한 해답이 완벽하다는 보장을 하기 힘들어집니다. 우리가 미처 찾지 못한 부분에 정답이 있을 수도 있으니까요. 하지만 그럴 듯한 답을 더 빠르고 효율적으로 찾을 수 있기도 합니다.

두 방식 다 나름의 장단점을 갖고 있습니다. 하지만 온갖 복잡한 법칙과 확률이 가득한 현실에서 모든 경우의 수를 고려하는 Model-based 방식은 명백한 한계를 갖게 됩니다.

앞으로 알아볼 방식이 주로 Model-Free 방식인 것 역시 같은 이유 때문입니다. 

아무 것도 없는 곳에서 스스로 학습하고 성장하는 인공지능, 미리 답을 주기보다는 스스로 답을 찾는 과정이 보다 인간의 학습과정을 닮아있기 때문이기도 하죠.

<br>

### 맺음말

강화학습의 최적화 대상으로서의 Model을 살펴보고, Model-based와 Model-Free의 차이를 비교해 보았습니다. 

하지만 아직 뭔가 부족하다는 생각이 드실 지도 모르겠습니다. 정작 Agent의 선택에 직접적인 영향을 주는 요소를 살피지 않았기 때문이죠. 

다음 포스트에선 이제 직접적으로 Agent에 영향을 주면서 동시에 또 다른 최적화 대상인 **Policy**와 **Value**에 대해 살펴보겠습니다.
